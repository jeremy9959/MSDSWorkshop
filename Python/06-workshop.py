#%%
import re

from collections import Counter

import numpy as np

#%%
# make a class for handling computation of term frequency matrix


class Tfbuilder:
    """
    A term-frequency matrix.

        Options:
        - word_pattern: a regular expression defining a word (default=r"\b\w\w+\b")
        - lowercase: convert all text to lower case (default=True)
        - stop_words: a list of words to ignore (or None). (default=None)
        - vocab_size: maximum size of vocabulary, starting from most common words, or None (no limit). (default=None)

        Attributes:
        - vocab: a dictionary with vocabulary words as keys and the corresponding column of the term frequency matrix as values
        - X: a term frequency matrix where entry i,j is number of occurences of the jth vocabulary word in the ith text

        Methods:
        - build_matrix: takes a list of texts as argument and builds the vocabulary and term frequency matrix
    """

    def __init__(
        self,
        word_pattern=r"\b\w\w+\b",
        lowercase=True,
        stop_words=None,
        vocab_size=None,
    ):
        self.pattern = word_pattern
        self.lowercase = lowercase
        if stop_words:
            self.stop_words = stop_words
        else:
            self.stop_words = []
        self.vocab_size = vocab_size

    def build_matrix(self, texts):
        "Build the term frequency matrix and the vocabulary from a list of texts"
        samples = {}
        vocab = {}
        total_freqs = Counter()
        for n, text in enumerate(texts):

            per_text_freqs = Counter()  # reset per text counter

            if self.lowercase:
                text = text.lower()

            for word_match in re.finditer(self.pattern, text):
                word = word_match[0]
                if word not in self.stop_words:
                    per_text_freqs[word] += 1
                    total_freqs[word] += 1
            samples[n] = per_text_freqs

        self.sorted_vocab = dict(
            sorted(total_freqs.items(), reverse=True, key=lambda x: x[1])
        )

        if not self.vocab_size:
            self.vocab_size = len(self.sorted_vocab)

        self.column_index = {
            x: i for i, x in enumerate(self.sorted_vocab) if i < self.vocab_size
        }

        X = np.zeros(shape=(len(samples), len(self.column_index)))

        for row, row_freqs in samples.items():
            for word, freq in row_freqs.items():
                if word in self.column_index:
                    X[row, self.column_index[word]] = row_freqs[word]

        self.vocab = self.column_index
        self.X = X


#%%


def main():
    par1 = """
    Lexers are generally quite simple, with most of the complexity deferred to the parser or semantic analysis phases, and can often be generated by a lexer generator, notably lex or derivatives. However, lexers can sometimes include some complexity, such as phrase structure processing to make input easier and simplify the parser, and may be written partly or fully by hand, either to support more features or for performance. 
    """
    par2 = """
    A lexer forms the first phase of a compiler frontend in processing. Analysis generally occurs in one pass.

    In older languages such as ALGOL, the initial stage was instead line reconstruction, which performed unstropping and removed whitespace and comments (and had scannerless parsers, with no separate lexer). These steps are now done as part of the lexer.

    Lexers and parsers are most often used for compilers, but can be used for other computer language tools, such as prettyprinters or linters. Lexing can be divided into two stages: the scanning, which segments the input string into syntactic units called lexemes and categorizes these into token classes; and the evaluating, which converts lexemes into processed values. 
    """
    par3 = """
    The word lexeme in computer science is defined differently than lexeme in linguistics. A lexeme in computer science roughly corresponds to a word in linguistics (not to be confused with a word in computer architecture), although in some cases it may be more similar to a morpheme. In some natural languages (for example, in English), the linguistic lexeme is similar to the lexeme in computer science, but this is generally not true (for example, in Chinese, it is highly non-trivial to find word boundaries due to the lack of word separators). 
    """

    texts = [par1, par2, par3]
    T = Tfbuilder()
    T.build_matrix(texts)
    return T


if __name__ == "__main__":
    T = main()
    print(T.vocab)
    print(T.X)


# %%
